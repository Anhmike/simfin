{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimFin Test All Datasets\n",
    "\n",
    "This Notebook performs automated testing of all the bulk datasets from SimFin. The datasets are first downloaded from the SimFin server and then various tests are performed on the data. An exception is raised if any problems are found.\n",
    "\n",
    "This Notebook can be run as usual if you have `simfin` installed, by running the following command from the directory where this Notebook is located:\n",
    "\n",
    "    jupyter notebook\n",
    "\n",
    "This Notebook can also be run using `pytest` which makes automated testing easier. You need to have the Python packages `simfin` and `nbval` installed. Then execute the following command from the directory where this Notebook is located:\n",
    "\n",
    "    pytest --nbval-lax -v test_bulk_data.ipynb\n",
    "    \n",
    "This runs the entire Notebook and outputs error messages for all the cells that raised an exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT!\n",
    "\n",
    "- When you make changes to this Notebook, remember to clear all cells before pushing it back to github, because that makes it easier to see the difference from the previous version. Select menu-item \"Kernel / Restart & Clear Output\".\n",
    "\n",
    "- If you set `refresh_days=0` then it will force a new download of all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to 0 to force a new download of all datasets.\n",
    "refresh_days = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simfin as sf\n",
    "from simfin.names import *\n",
    "from simfin.datasets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are We Running Pytest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean whether this is being run under pytest.\n",
    "# This is useful when printing examples of errors\n",
    "# if they take a long time to compute, because it\n",
    "# is not necessary when running pytest.\n",
    "running_pytest = ('PYTEST_CURRENT_TEST' in os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure SimFin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.set_data_dir('~/simfin_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.load_api_key(path='~/simfin_api_key.txt', default_key='free')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = AllDatasets(refresh_days=refresh_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for annual Income Statements.\n",
    "data.get(dataset='income', variant='annual', market='us').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists of Datasets\n",
    "\n",
    "These are in addition to the lists of datasets from `datasets.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets that have a column named TICKER.\n",
    "# Some tests are probably only necessary for 'companies'\n",
    "# but we might as well test all datasets that use tickers.\n",
    "datasets_tickers = ['companies'] + datasets_fundamental() + datasets_shareprices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_datasets(test_name, datasets=None, variants=None,\n",
    "                  markets=None,\n",
    "                  test_func=None,\n",
    "                  test_func_rows=None,\n",
    "                  test_func_groups=None,\n",
    "                  group_index=SIMFIN_ID,\n",
    "                  process_df_none=False, raise_exception=True):\n",
    "    \"\"\"\n",
    "    Helper-function for running tests on many Pandas DataFrames.\n",
    "    \n",
    "    :param test_name:\n",
    "        String with the name of the test.\n",
    "        \n",
    "    :param datasets:\n",
    "        By default (datasets=None) all possible datasets\n",
    "        will be tested. Otherwise datasets is a list of\n",
    "        strings with dataset names to be tested.\n",
    "        \n",
    "    :param variants:\n",
    "        By default (variants=None) all possible variants\n",
    "        for each dataset will be tested, as defined in\n",
    "        simfin.datasets.valid_variants. Otherwise variants\n",
    "        is a list of strings and only those variants\n",
    "        will be tested.\n",
    "        \n",
    "    :param markets:\n",
    "        By default (markets=None) all possible markets\n",
    "        for each dataset will be tested, as defined in\n",
    "        simfin.datasets.valid_markets. Otherwise markets\n",
    "        is a list of strings and only those markets\n",
    "        will be tested.\n",
    "        \n",
    "    :param test_func:\n",
    "        Function to be called on the Pandas DataFrame for\n",
    "        each dataset. If there are problems with the DataFrame\n",
    "        then return True, otherwise return False.\n",
    "        \n",
    "        This is generally used for testing problems with the\n",
    "        entire DataFrame. For example, if the dataset is empty:\n",
    "\n",
    "        test_func = lambda df: len(df) == 0\n",
    "        \n",
    "        If this returns True then there is a problem with df.\n",
    "                \n",
    "    :param test_func_rows:\n",
    "        Similar to test_func but for testing individual rows\n",
    "        of a DataFrame. For example, test if SHARES_BASIC is\n",
    "        None, zero or negative:\n",
    "        \n",
    "        test_func_rows = lambda df: (df[SHARES_BASIC] is None or\n",
    "                                     df[SHARES_BASIC] <= 0)\n",
    "\n",
    "    :param test_func_groups:\n",
    "        Similar to test_func but for testing groups of rows\n",
    "        in a DataFrame. For example, test on a per-stock basis\n",
    "        whether SHARES_BASIC is greater than twice its mean:\n",
    "        \n",
    "        test_func_groups = lambda df: (df[SHARES_BASIC] >\n",
    "                                       df[SHARES_BASIC].mean() * 2).any()\n",
    "\n",
    "    :param group_index:\n",
    "        String with the column-name used to create groups when\n",
    "        using test_func_groups e.g. SIMFIN_ID for grouping by companies.\n",
    "\n",
    "    :param process_df_none:\n",
    "        Boolean whether to process (True) or skip (False)\n",
    "        DataFrames that are None, because they could not be loaded.\n",
    "\n",
    "    :param raise_exception:\n",
    "        Boolean. If True then raise an exception if there were\n",
    "        any problems, but wait until all datasets have been\n",
    "        tested, so we can print the list of datasets with problems.\n",
    "        If False then only show a warning if there were problems.\n",
    "        \n",
    "    :return:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to test_func.\n",
    "    if test_func_rows is not None:\n",
    "        # Convert test_func_rows to test_func.\n",
    "        test_func = lambda df: test_func_rows(df).any()\n",
    "    elif test_func_groups is not None:\n",
    "        # Convert test_func_groups to test_func.\n",
    "        # NOTE: We must use .any(axis=None) because if the DataFrame\n",
    "        # is empty then the groupby returns an empty DataFrame, and\n",
    "        # .any() then returns an empty Series, but we need a boolean.\n",
    "        # By using .any(axis=None) it is reduced to a boolean value.\n",
    "        test_func = lambda df: df.groupby(group_index, group_keys=False).apply(test_func_groups).any(axis=None)\n",
    "\n",
    "    # Number of problems found.\n",
    "    num_problems = 0\n",
    "\n",
    "    # For all datasets, variants and markets.\n",
    "    for dataset, variant, market, df in data.iter(datasets=datasets,\n",
    "                                                  variants=variants,\n",
    "                                                  markets=markets):\n",
    "        # Also process DataFrames that are None,\n",
    "        # because they could not be loaded?\n",
    "        if df is not None or process_df_none:\n",
    "            try:\n",
    "                # Perform the user-supplied test.\n",
    "                problem_found = test_func(df)\n",
    "            except:\n",
    "                # An exception occurred so we consider\n",
    "                # that to be a problem.\n",
    "                problem_found = True\n",
    "                \n",
    "            if problem_found:\n",
    "                # Increase the number of problems found.\n",
    "                num_problems += 1\n",
    "\n",
    "                # Print the test's name. Only done once.\n",
    "                if num_problems==1:\n",
    "                    print(test_name, file=sys.stderr)\n",
    "\n",
    "                # Print the dataset details.\n",
    "                msg = \"dataset='{}', variant='{}', market='{}'\"\n",
    "                msg = msg.format(dataset, variant, market)\n",
    "                print(msg, file=sys.stderr)\n",
    "                \n",
    "    # Raise exception or generate warning?\n",
    "    if num_problems>0:\n",
    "        if raise_exception:\n",
    "            raise Exception(test_name)\n",
    "        else:\n",
    "            warnings.warn(test_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Getting Rows with Problems\n",
    "\n",
    "When a test has found problems in a dataset, it does not show which specific rows have the problem. You can get all the problematic rows using this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_problem_rows(df, test_func_rows):\n",
    "    \"\"\"\n",
    "    Perform the given test on all rows of the given DataFrame\n",
    "    and return a DataFrame with only the problematic rows.\n",
    "    \n",
    "    :param df:\n",
    "        Pandas DataFrame.\n",
    "\n",
    "    :param test_func_rows:\n",
    "        Function used for testing each row. This takes\n",
    "        a Pandas DataFrame as an argument and returns\n",
    "        a Pandas Series of booleans whether each row\n",
    "        in the original DataFrame has the error.\n",
    "        \n",
    "        For example:\n",
    "        \n",
    "        test_func_rows = lambda df: (df[SHARES_BASIC] is None or\n",
    "                                     df[SHARES_BASIC] <= 0)\n",
    "\n",
    "    :return:\n",
    "        Pandas DataFrame with only the problematic rows.\n",
    "    \"\"\"\n",
    "\n",
    "    # Index of the rows with problems.\n",
    "    idx = test_func_rows(df)\n",
    "    \n",
    "    # Extract the rows with problems.\n",
    "    df2 = df[idx]\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Getting Rows with Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_data_rows(df, column):\n",
    "    \"\"\"\n",
    "    Return the rows of `df` where the data for the given\n",
    "    column is missing i.e. it is either NaN, None, or Null.\n",
    "    \n",
    "    :param df:\n",
    "        Pandas DataFrame.\n",
    "    \n",
    "    :param column:\n",
    "        Name of the column.\n",
    "\n",
    "    :return:\n",
    "        Pandas Series with the rows where the\n",
    "        column-data is missing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Index for the rows where column-data is missing.\n",
    "    idx = df[column].isnull()\n",
    "\n",
    "    # Get those rows from the DataFrame.\n",
    "    df2 = df[idx]\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Getting Problematic Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_problem_groups(df, test_func_groups, group_index):\n",
    "    \"\"\"\n",
    "    Perform the given test on the given DataFrame grouped by\n",
    "    the given index, and return a DataFrame with only the\n",
    "    problematic groups.\n",
    "    \n",
    "    This is used to perform tests on a DataFrame on a per-group\n",
    "    basis, e.g. per-stock or per-company, and return a new\n",
    "    DataFrame with only the rows for the stocks that had problems.\n",
    "    \n",
    "    :param df:\n",
    "        Pandas DataFrame.\n",
    "\n",
    "    :param test_func_groups:\n",
    "        Similar to test_func but for testing groups of rows\n",
    "        in a DataFrame. For example, test on a per-stock basis\n",
    "        whether SHARES_BASIC is greater than twice its mean:\n",
    "        \n",
    "        test_func_groups = lambda df: (df[SHARES_BASIC] >\n",
    "                                       df[SHARES_BASIC].mean() * 2)\n",
    "\n",
    "    :param group_index:\n",
    "        String with the column-name used to create groups when\n",
    "        using test_func_groups e.g. SIMFIN_ID for grouping by companies.\n",
    "\n",
    "    :return:\n",
    "        Pandas DataFrame with only the problematic groups.\n",
    "    \"\"\"\n",
    "\n",
    "    return df.groupby(group_index).filter(test_func_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Testing Equality with Tolerance\n",
    "\n",
    "This function is useful when comparing floating point numbers, or when comparing accounting numbers that are supposed to have a strict relationship (e.g. Assets = Liabilities + Equity) but we might tolerate a small degree of error in the data e.g. 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isclose(x, y, tolerance=0.01):\n",
    "    \"\"\"\n",
    "    Compare whether x and y are approximately equal within\n",
    "    the given tolerance, which is a ratio so tolerance=0.01\n",
    "    means that we tolerate max 1% difference between x and y.\n",
    "    \n",
    "    This is similar to numpy.isclose() but is a more efficient\n",
    "    implementation for Pandas which apparently does not have\n",
    "    this built-in already (v. 0.25.1)\n",
    "    \n",
    "    :param x:\n",
    "        Pandas DataFrame or Series.\n",
    "\n",
    "    :param y:\n",
    "        Pandas DataFrame or Series.\n",
    "\n",
    "    :param tolerance:\n",
    "        Max allowed difference as a ratio e.g. 0.01 = 1%.\n",
    "\n",
    "    :return:\n",
    "        Pandas DataFrame or Series with booleans whether\n",
    "        x and y are approx. equal.\n",
    "    \"\"\"\n",
    "    return (x-y).abs() <= tolerance * y.abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset could not be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"Dataset could not be loaded\"\n",
    "test_func = lambda df: df is None\n",
    "test_datasets(datasets=datasets_all(),\n",
    "              test_name=test_name, test_func=test_func,\n",
    "              process_df_none=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"Dataset is empty\"\n",
    "test_func = lambda df: len(df) == 0\n",
    "\n",
    "# Test for all markets. This only raises a warning,\n",
    "# because some markets do have some of their datasets empty.\n",
    "test_datasets(datasets=datasets_all(),\n",
    "              test_name=test_name, test_func=test_func,\n",
    "              raise_exception=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test only for the 'us' market. This raises an exception.\n",
    "# It happened once that all the datasets were empty\n",
    "# because of some bug on the server or whatever, so it\n",
    "# is important to raise an exception in case this happens again.\n",
    "test_datasets(datasets=datasets_all(), markets=['us'],\n",
    "              test_name=test_name, test_func=test_func,\n",
    "              raise_exception=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.get(dataset='income-insurance', variant='quarterly', market='de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shares Basic is None or <= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"SHARES_BASIC is None or <= 0\"\n",
    "test_func_rows = lambda df: (df[SHARES_BASIC] is None or\n",
    "                             df[SHARES_BASIC] <= 0)\n",
    "test_datasets(datasets=datasets_fundamental(),\n",
    "              test_name=test_name, test_func_rows=test_func_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the problematic rows for a dataset.\n",
    "df = data.get(dataset='income', variant='annual', market='us')\n",
    "get_problem_rows(df=df, test_func_rows=test_func_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shares Diluted is None or <= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_name = \"SHARES_DILUTED is None or <= 0\"\n",
    "test_func_rows = lambda df: (df[SHARES_DILUTED] is None or\n",
    "                             df[SHARES_DILUTED] <= 0)\n",
    "test_datasets(datasets=datasets_fundamental(),\n",
    "              test_name=test_name, test_func_rows=test_func_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the problematic rows for a dataset.\n",
    "df = data.get(dataset='income', variant='annual', market='us')\n",
    "get_problem_rows(df=df, test_func_rows=test_func_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shares Basic or Diluted looks strange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of SimFin-Id's to ignore in this test.\n",
    "# Use this list when a company's share-counts look strange,\n",
    "# but after manual inspection of the financial reports, the\n",
    "# share-counts are actually correct.\n",
    "ignore_simfin_ids = \\\n",
    "    [520475, 652016, 951586, 698616, 543421, 82753]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_groups(df_grp):\n",
    "    # Perform various tests on the share-counts.\n",
    "    # Assume `df_grp` only contains data for a single company,\n",
    "    # because this function should be called using:\n",
    "    # df.groupby(SIMFIN_ID).apply(test_func_groups)\n",
    "    \n",
    "    # Ignore this company?\n",
    "    if df_grp[SIMFIN_ID].iloc[0] in ignore_simfin_ids:\n",
    "        return False\n",
    "    \n",
    "    # Helper-function for calculating absolute ratio between\n",
    "    # a value and its average.\n",
    "    abs_ratio = lambda df: (df / df.mean() - 1).abs()\n",
    "\n",
    "    # Max absolute ratio allowed.\n",
    "    max_abs_ratio = 2\n",
    "    \n",
    "    # Test whether Shares Basic is much different from its mean.\n",
    "    test1 = (abs_ratio(df_grp[SHARES_BASIC]) > max_abs_ratio).any()\n",
    "\n",
    "    # Test whether Shares Diluted is much different from its mean.\n",
    "    test2 = (abs_ratio(df_grp[SHARES_DILUTED]) > max_abs_ratio).any()\n",
    "\n",
    "    return (test1 | test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_name = \"Shares Basic or Shares Diluted looks strange\"\n",
    "test_datasets(datasets=datasets_fundamental(),\n",
    "              test_name=test_name,\n",
    "              test_func_groups=test_func_groups,\n",
    "              group_index=SIMFIN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the problematic groups for a dataset.\n",
    "if not running_pytest:\n",
    "    # Get the dataset.\n",
    "    df = data.get(dataset='income', variant='annual', market='us')\n",
    "\n",
    "    # Get the problematic groups.\n",
    "    df_problems = get_problem_groups(df=df,\n",
    "                                     test_func_groups=test_func_groups,\n",
    "                                     group_index=SIMFIN_ID)\n",
    "\n",
    "    # Print the problematic groups.\n",
    "    for _, df2 in df_problems.groupby(SIMFIN_ID):\n",
    "        display(df2[[SIMFIN_ID, REPORT_DATE, SHARES_BASIC, SHARES_DILUTED]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revenue is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_name = \"REVENUE < 0\"\n",
    "test_func_rows = lambda df: (df[REVENUE] < 0)\n",
    "\n",
    "# It is possible that Revenue is negative for banks and\n",
    "# insurance companies, so we only test it for \"normal\" companies\n",
    "# in the 'income' dataset.\n",
    "test_datasets(datasets=['income'],\n",
    "              test_name=test_name, test_func_rows=test_func_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the problematic rows for a dataset.\n",
    "df = data.get(dataset='income-insurance', variant='quarterly', market='us')\n",
    "get_problem_rows(df=df, test_func_rows=test_func_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assets != Liabilities + Equity (Exact Comparison)\n",
    "\n",
    "This only generates a warning, because sometimes there are tiny rounding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_name = \"Assets != Liabilities + Equity (Exact Comparison)\"\n",
    "test_func_rows = lambda df: (df[TOTAL_ASSETS] != df[TOTAL_LIABILITIES] + df[TOTAL_EQUITY])\n",
    "test_datasets(datasets=datasets_balance(),\n",
    "              test_name=test_name, test_func_rows=test_func_rows,\n",
    "              raise_exception=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the problematic rows for a dataset.\n",
    "df = data.get(dataset='balance', variant='quarterly', market='us')\n",
    "df2 = get_problem_rows(df=df, test_func_rows=test_func_rows)\n",
    "\n",
    "# Only show the relevant columns.\n",
    "df2[[TICKER, SIMFIN_ID, REPORT_DATE, TOTAL_ASSETS, TOTAL_LIABILITIES, TOTAL_EQUITY]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assets != Liabilities + Equity (1% Tolerance)\n",
    "\n",
    "The above test used exact comparison. We now allow for 1% error. This raises an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_rows(df):\n",
    "    x = df[TOTAL_ASSETS]\n",
    "    y = df[TOTAL_LIABILITIES] + df[TOTAL_EQUITY]\n",
    "    \n",
    "    # Compare x and y within 1% tolerance. Note the resulting\n",
    "    # boolean array is negated because we want to indicate\n",
    "    # which rows are problematic so x and y are not close.\n",
    "    return ~isclose(x=x, y=y, tolerance=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_name = \"Assets != Liabilities + Equity (1% Tolerance)\"\n",
    "test_datasets(datasets=datasets_balance(),\n",
    "              test_name=test_name, test_func_rows=test_func_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the problematic rows for a dataset.\n",
    "df = data.get(dataset='balance', variant='annual', market='us')\n",
    "df2 = get_problem_rows(df=df, test_func_rows=test_func_rows)\n",
    "\n",
    "# Only show the relevant columns.\n",
    "df2[[TICKER, SIMFIN_ID, REPORT_DATE, TOTAL_ASSETS, TOTAL_LIABILITIES, TOTAL_EQUITY]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates are invalid (Fundamentals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda function for converting strings to dates. Format: YYYY-MM-DD\n",
    "# This will raise an exception if invalid dates are encountered.\n",
    "date_parser = lambda column: pd.to_datetime(column, yearfirst=True, dayfirst=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for the entire DataFrame.\n",
    "# This cannot show which individual rows have problems.\n",
    "def test_func(df):\n",
    "    result1 = date_parser(df[REPORT_DATE])\n",
    "    result2 = date_parser(df[PUBLISH_DATE])\n",
    "    \n",
    "    # We only get to this point if date_parser() does not\n",
    "    # raise any exceptions, in which case we assume the\n",
    "    # data did not have any problems.\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"REPORT_DATE or PUBLISH_DATE is invalid\"\n",
    "test_datasets(datasets=datasets_fundamental(),\n",
    "              test_name=test_name, test_func=test_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates are invalid (Share-Prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for the entire DataFrame.\n",
    "# This cannot show which individual rows have problems.\n",
    "def test_func(df):\n",
    "    result1 = date_parser(df[DATE])\n",
    "    \n",
    "    # We only get to this point if date_parser() does not\n",
    "    # raise any exceptions, in which case we assume the\n",
    "    # data did not have any problems.\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"DATE is invalid\"\n",
    "test_datasets(datasets=datasets_shareprices(),\n",
    "              test_name=test_name, test_func=test_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duplicate_tickers(df):\n",
    "    \"\"\"\n",
    "    Return the rows of `df` where multiple SIMFIN_ID\n",
    "    have the same TICKER.\n",
    "    \n",
    "    :param df: Pandas DataFrame with TICKER column.\n",
    "    :return: Pandas DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove duplicate rows of [TICKER, SIMFIN_ID] pairs.\n",
    "    # For the 'companies' dataset this is not necessary,\n",
    "    # but for e.g. the 'income' dataset we have many rows\n",
    "    # for each [TICKER, SIMFIN_ID] pair because there are\n",
    "    # many financial reports for each of these ID pairs.\n",
    "    idx = df[[TICKER, SIMFIN_ID]].duplicated()\n",
    "    df2 = df[~idx]\n",
    "\n",
    "    # Now the DataFrame df2 only contains unique rows of\n",
    "    # [TICKER, SIMFIN_ID] so we need to check if there are\n",
    "    # any duplicate TICKER.\n",
    "\n",
    "    # Index for rows where TICKER is a duplicate.\n",
    "    idx1 = df2[TICKER].duplicated()\n",
    "\n",
    "    # Index for rows where TICKER is not NaN.\n",
    "    # These would otherwise show up as duplicates.\n",
    "    idx2 = df2[TICKER].notna()\n",
    "\n",
    "    # Index for rows where TICKER is a duplicate but not NaN.\n",
    "    idx = idx1 & idx2\n",
    "\n",
    "    # Get those rows from the DataFrame.\n",
    "    df2 = df2[idx]\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-function whether a DataFrame has duplicate tickers.\n",
    "test_func = lambda df: (len(get_duplicate_tickers(df=df)) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"Duplicate Tickers\"\n",
    "test_datasets(datasets=datasets_tickers,\n",
    "              test_name=test_name, test_func=test_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show duplicate tickers in the 'companies' dataset.\n",
    "df = data.get(dataset='companies', market='us')\n",
    "get_duplicate_tickers(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show duplicate tickers in the 'income-annual' dataset.\n",
    "df = data.get(dataset='income', variant='annual', market='us')\n",
    "get_duplicate_tickers(df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-function whether a DataFrame has missing tickers.\n",
    "test_func = lambda df: (len(get_missing_data_rows(df=df, column=TICKER)) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"Missing Tickers\"\n",
    "test_datasets(datasets=datasets_tickers,\n",
    "              test_name=test_name, test_func=test_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show missing tickers in the 'companies' dataset.\n",
    "df = data.get(dataset='companies', market='us')\n",
    "get_missing_data_rows(df=df, column=TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show missing tickers in the 'income-annual' dataset.\n",
    "df = data.get(dataset='income', variant='annual', market='us')\n",
    "get_missing_data_rows(df=df, column=TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show missing tickers in the 'shareprices-daily' dataset.\n",
    "df = data.get(dataset='shareprices', variant='daily', market='us')\n",
    "get_missing_data_rows(df=df, column=TICKER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Company Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-function whether a DataFrame has missing company names.\n",
    "test_func = lambda df: (len(get_missing_data_rows(df=df, column=COMPANY_NAME)) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"Missing Company Name\"\n",
    "test_datasets(datasets=['companies'],\n",
    "              test_name=test_name, test_func=test_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show missing company names in the 'companies' dataset.\n",
    "df = data.get(dataset='companies', market='us')\n",
    "get_missing_data_rows(df=df, column=COMPANY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Annual Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_annual_reports(df):\n",
    "    \"\"\"\n",
    "    Return a list of the SIMFIN_ID's from the given DataFrame\n",
    "    that have missing annual reports.\n",
    "    \n",
    "    :param df:\n",
    "        Pandas DataFrame with a dataset e.g. 'income-annual'.\n",
    "        It must have columns SIMFIN_ID and FISCAL_YEAR.\n",
    "\n",
    "    :return:\n",
    "        List of integers with SIMFIN_ID's that have missing reports.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The idea is to test for each SIMFIN_ID individually,\n",
    "    # whether the DataFrame has all the expected reports for\n",
    "    # consecutive Fiscal Years between the min/max years.\n",
    "    \n",
    "    # Helper-function for processing a DataFrame for one SIMFIN_ID.\n",
    "    def _missing(df):\n",
    "        # Get the Fiscal Years from the DataFrame.\n",
    "        fiscal_years = df[FISCAL_YEAR]\n",
    "\n",
    "        # How many years between min and max fiscal years.\n",
    "        num_years = fiscal_years.max() - fiscal_years.min() + 1\n",
    "\n",
    "        # We expect the Series to have the same length, otherwise\n",
    "        # some reports must be missing between min and max years.\n",
    "        missing = (num_years != len(fiscal_years))\n",
    "\n",
    "        return missing\n",
    "    \n",
    "    # Process all companies individually and get a Pandas\n",
    "    # DataFrame with a boolean for each SIMFIN_ID whether\n",
    "    # it has some missing Fiscal Years.\n",
    "    idx = df.groupby(SIMFIN_ID).apply(_missing)\n",
    "\n",
    "    # List of the SIMFIN_ID's that have missing reports.\n",
    "    simfin_ids = list(idx[idx].index.values)\n",
    "\n",
    "    return simfin_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"Missing annual reports\"\n",
    "test_func = lambda df: len(missing_annual_reports(df=df)) > 0\n",
    "test_datasets(datasets=datasets_fundamental(),\n",
    "              variants=['annual'],\n",
    "              test_name=test_name, test_func=test_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of SIMFIN_ID's that have missing reports for a dataset.\n",
    "if not running_pytest:\n",
    "    df = data.get(dataset='income', variant='annual', market='de')\n",
    "    display(missing_annual_reports(df=df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_annual_reports(df, simfin_id):\n",
    "    \"\"\"\n",
    "    Get the data for a given SIMFIN_ID and set the index to be\n",
    "    the sorted Fiscal Year so it is easier to see which are missing.\n",
    "    \"\"\"\n",
    "    return df.set_index([SIMFIN_ID, FISCAL_YEAR]).sort_index().loc[simfin_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all the reports for a given SIMFIN_ID sorted by\n",
    "# Fiscal Year so it is easier to see which are missing.\n",
    "if not running_pytest:\n",
    "    display(sort_annual_reports(df=df, simfin_id=936426))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Quarterly Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_quarterly_reports(df):\n",
    "    \"\"\"\n",
    "    Return a list of the SIMFIN_ID's from the given DataFrame\n",
    "    that have missing quarterly or ttm reports.\n",
    "    \n",
    "    :param df:\n",
    "        Pandas DataFrame with a dataset e.g. 'income-annual'.\n",
    "        It must have columns SIMFIN_ID, FISCAL_YEAR, FISCAL_PERIOD.\n",
    "\n",
    "    :return:\n",
    "        List of integers with SIMFIN_ID's that have missing reports.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The idea is to test for each SIMFIN_ID individually,\n",
    "    # whether the DataFrame has all the expected reports for\n",
    "    # consecutive Fiscal Years and Periods between the min/max.\n",
    "    \n",
    "    # Helper-function for processing a DataFrame for one SIMFIN_ID.\n",
    "    def _missing(df):\n",
    "        # Get the Fiscal Years and Periods from the DataFrame.\n",
    "        fiscal_years_periods = df[[FISCAL_YEAR, FISCAL_PERIOD]]\n",
    "\n",
    "        # The first Fiscal Year and Period.\n",
    "        min_year = fiscal_years_periods[FISCAL_YEAR].min()\n",
    "        min_idx = (fiscal_years_periods[FISCAL_YEAR] == min_year)\n",
    "        min_period = fiscal_years_periods[min_idx][FISCAL_PERIOD].min()\n",
    "\n",
    "        # The last Fiscal Year and Period.\n",
    "        max_year = fiscal_years_periods[FISCAL_YEAR].max()\n",
    "        max_idx = (fiscal_years_periods[FISCAL_YEAR] == max_year)\n",
    "        max_period = fiscal_years_periods[max_idx][FISCAL_PERIOD].max()\n",
    "\n",
    "        # How many years between min and max fiscal years.\n",
    "        num_years = max_year - min_year + 1\n",
    "\n",
    "        # Total number of Fiscal Periods between first and\n",
    "        # last Fiscal Years - if all Fiscal Periods were included.\n",
    "        num_periods = num_years * 4\n",
    "\n",
    "        # Used to map from Fiscal Period strings to ints.\n",
    "        # This is safer and easier to understand than\n",
    "        # e.g. def map_period(x): int(x[1])\n",
    "        map_period = \\\n",
    "        {\n",
    "            'Q1': 1,\n",
    "            'Q2': 2,\n",
    "            'Q3': 3,\n",
    "            'Q4': 4\n",
    "        }\n",
    "\n",
    "        # Number of Fiscal Periods missing in the first year.\n",
    "        adj_min_period = map_period[min_period] - 1\n",
    "\n",
    "        # Number of Fiscal Periods missing in the last year.\n",
    "        adj_max_period = 4 - map_period[max_period]\n",
    "\n",
    "        # Adjust the number of Fiscal Periods between the min/max\n",
    "        # Fiscal Years and Periods by subtracting those periods\n",
    "        # missing in the first and last years.\n",
    "        expected_periods = num_periods - adj_min_period - adj_max_period\n",
    "\n",
    "        # If the expected number of Fiscal Periods between the\n",
    "        # min and max dates, is different from the actual number\n",
    "        # of Fiscal Periods in the DataFrame, then some are missing.\n",
    "        missing = (expected_periods != len(fiscal_years_periods))\n",
    "\n",
    "        return missing\n",
    "\n",
    "    # Process all companies individually and get a Pandas\n",
    "    # DataFrame with a boolean for each SIMFIN_ID whether\n",
    "    # it has some missing Fiscal Years.\n",
    "    idx = df.groupby(SIMFIN_ID).apply(_missing)\n",
    "\n",
    "    # List of the SIMFIN_ID's that have missing reports.\n",
    "    simfin_ids = list(idx[idx].index.values)\n",
    "\n",
    "    return simfin_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_name = \"Missing quarterly reports\"\n",
    "test_func = lambda df: len(missing_quarterly_reports(df=df)) > 0\n",
    "test_datasets(datasets=datasets_fundamental(),\n",
    "              variants=['quarterly'],\n",
    "              test_name=test_name, test_func=test_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of SIMFIN_ID's that have missing reports for a dataset.\n",
    "if not running_pytest:\n",
    "    df = data.get(dataset='income', variant='quarterly', market='us')\n",
    "    display(missing_quarterly_reports(df=df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_quarterly_reports(df, simfin_id):\n",
    "    \"\"\"\n",
    "    Get the data for a given SIMFIN_ID and set the index to be\n",
    "    the sorted Fiscal Year and Period so it is easier to see\n",
    "    which ones are missing.\n",
    "    \"\"\"\n",
    "    return df.set_index([SIMFIN_ID, FISCAL_YEAR, FISCAL_PERIOD]).sort_index().loc[simfin_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show all the reports for a given SIMFIN_ID sorted by\n",
    "# Fiscal Year and Period so it is easier to see which are missing.\n",
    "if not running_pytest:\n",
    "    display(sort_quarterly_reports(df=df, simfin_id=139560))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing TTM Reports\n",
    "\n",
    "Trailing-Twelve-Months (TTM) data is also quarterly so we can use the same helper-functions from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"Missing ttm reports\"\n",
    "test_func = lambda df: len(missing_quarterly_reports(df=df)) > 0\n",
    "test_datasets(datasets=datasets_fundamental(),\n",
    "              variants=['ttm'],\n",
    "              test_name=test_name, test_func=test_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of SIMFIN_ID's that have missing reports for a dataset.\n",
    "if not running_pytest:\n",
    "    df = data.get(dataset='income', variant='ttm', market='us')\n",
    "    display(missing_quarterly_reports(df=df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show all the reports for a given SIMFIN_ID sorted by\n",
    "# Fiscal Year and Period so it is easier to see which are missing.\n",
    "if not running_pytest:\n",
    "    display(sort_quarterly_reports(df=df, simfin_id=89750))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
